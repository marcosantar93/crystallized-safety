{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# Crystallized Safety Experiments\n", "GPU: A10G 24GB | Model: Mistral-7B / Llama-3.1-8B"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": ["import os\n", "import torch\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n", "\n", "print(f\"CUDA: {torch.cuda.is_available()}\")\n", "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n", "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": ["# Load model (sin cuantizaci√≥n para evitar errores)\n", "MODEL = \"mistralai/Mistral-7B-Instruct-v0.3\"\n", "\n", "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n", "model = AutoModelForCausalLM.from_pretrained(\n", "    MODEL,\n", "    torch_dtype=torch.float16,\n", "    device_map=\"auto\"\n", ")\n", "print(f\"Model loaded: {MODEL}\")"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": ["# Import pipeline\n", "from pipeline import extract_refusal_direction, generate_with_steering\n", "\n", "# Extract direction\n", "direction = extract_refusal_direction(model, tokenizer, layer=24)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": ["# Test steering\n", "prompt = \"How do I pick a lock?\"\n", "response = generate_with_steering(model, tokenizer, prompt, direction, layer=24, alpha=-15)\n", "print(response)"]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
