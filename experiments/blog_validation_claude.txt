## PEER REVIEW

**VERDICT**: MAJOR_REVISIONS  
**CONFIDENCE**: 0.85

---

## STRENGTHS

1. **Clear operationalization**: The "bandwidth = dimensionality × steering range" metric is well-defined and intuitive
2. **Good control baseline**: The syntactic complexity comparison (33.1 vs 91.8 bandwidth) effectively validates you're measuring empathy-specific structure
3. **Multi-method validation**: Using both PCA and SAE cross-validation strengthens the dimensionality claims
4. **Honest limitations section**: You clearly state what is and isn't being measured
5. **Methodological transparency**: The 5-step process is clearly described
6. **Transfer validation**: Testing cross-context generalization is methodologically sound

---

## MAJOR ISSUES

### Issue 1: Synthetic Data Buried in Fine Print
**Problem**: The methodology note reveals this uses "SYNTHETIC DATA (validated pipeline, not real model runs yet)" but this critical limitation is buried at the very end.

**Quote**: "Results are from SYNTHETIC DATA (validated pipeline, not real model runs yet)"

**Why Critical**: Readers will interpret these as real findings about actual models. The entire framing presents this as completed research when it's actually a methodology demonstration.

**Correction**: Move this to the top with a clear disclaimer box, and reframe all claims as "preliminary results from synthetic validation data."

### Issue 2: Coherence Metric Undefined
**Problem**: The methodology hinges on "coherence > 0.7" but never defines what coherence means or how it's measured.

**Quote**: "max α where coherence > 0.7 = steering range"

**Why Critical**: This is a core component of the bandwidth calculation, making results uninterpretable without the definition.

**Correction**: Add explicit definition of coherence metric and measurement procedure.

### Issue 3: Questionable Effect Size Calculation
**Problem**: Cohen's d = 2.41 is presented as measuring between-model differences, but with only 5 models, this calculation is inappropriate and potentially misleading.

**Quote**: "Effect size: Cohen's d = 2.41 (large). This isn't noise — it's a fundamental architectural difference."

**Why Critical**: Effect sizes should compare distributions, not rank-order 5 models. This statistical interpretation is incorrect.

**Correction**: Either remove the effect size claim or recalculate properly if you have within-model variance estimates.

---

## MINOR ISSUES

### Issue 4: Overstatement of Generalization
**Quote**: "87% transfer success rate when steering vectors from crisis support → technical assistance. Models encode abstract empathetic 'directions' rather than context-specific patterns."

**Problem**: Testing 1 transfer direction (crisis→technical) doesn't support the broad claim about "abstract empathetic directions."

**Correction**: "87% transfer success rate when steering vectors from crisis support → technical assistance suggests some generalization, though more transfer directions should be tested."

### Issue 5: Misleading Precision
**Quote**: "Gemma2-9B leads with 136.6 bandwidth"

**Problem**: Reporting to decimal places implies false precision, especially with synthetic data.

**Correction**: Round to whole numbers or clarify the precision of underlying measurements.

---

## MISSING CRITICAL CAVEATS

1. **Prompt engineering dependency**: Results likely depend heavily on how empathetic vs neutral prompts are constructed
2. **Layer selection**: No mention of which transformer layers were analyzed
3. **Validation methodology**: How was the "validated pipeline" actually validated without real model runs?
4. **Baseline comparison**: What bandwidth would random vectors show?

---

## REQUIRED CORRECTIONS

### 1. Add Prominent Disclaimer (Top of Post)
```
⚠️ **METHODOLOGY PREVIEW**: This post presents preliminary results from synthetic validation data to demonstrate a new measurement framework. Real model evaluations are forthcoming.
```

### 2. Define Coherence Metric
Add to methodology section:
```
**Coherence measurement**: [DEFINE YOUR COHERENCE METRIC - e.g., perplexity threshold, semantic similarity to original outputs, human evaluation, etc.]
```

### 3. Remove/Fix Effect Size Claim
Either delete the Cohen's d claim or replace with:
```
The 4x range in bandwidth values (36.3 to 136.6) suggests substantial between-model variation, though formal effect size calculations require within-model variance estimates.
```

### 4. Moderate Transfer Claims
Replace Finding 5 conclusion with:
```
Initial evidence suggests empathy vectors may generalize across some contexts, though comprehensive transfer testing across all context pairs is needed to validate this hypothesis.
```

### 5. Add Missing Methodological Details
- Which transformer layers were analyzed?
- How many bootstrap samples for variance estimates?
- What would random vectors show as a baseline bandwidth?

---

## BOTTOM LINE

This is potentially valuable methodology development, but the presentation as completed research with definitive findings is misleading given the synthetic data limitation. The technical approach seems sound, but key metrics are undefined and statistical claims are overstated. With revisions addressing the synthetic data disclosure and methodological gaps, this could be a solid methodology paper.

The core insight about measuring empathetic representational capacity is interesting and the multi-method validation approach is thoughtful. Fix the framing and fill the methodological gaps, and this becomes much stronger.