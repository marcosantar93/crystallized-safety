\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{centernot}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{multirow}

% arXiv prefers natbib
\usepackage{natbib}
\bibliographystyle{plainnat}

\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{gray!10}
}

\title{Crystallized Safety: Why Readable Representations \\
Don't Mean Controllable Behavior in LLMs}

\author{
Marco Santarcangelo \\
Scale AI \\
\texttt{marco@marcosantar.com} \\
\url{https://github.com/marcosantar93}
}

\date{January 2026}

\begin{document}

\maketitle

\begin{abstract}
We introduce \textbf{crystallized safety}---the phenomenon where safety-relevant concepts are geometrically represented in LLM activation space (readable via probing) yet resist manipulation via activation steering (not controllable). Through systematic experiments across three model families (Gemma-2, Llama-3, Mistral) totaling 36+ configurations (varying layers and steering magnitudes $\alpha \in [-10, +15]$), we demonstrate 0\% behavioral flip rate despite successful direction extraction and coherence preservation. This paradox---readable $\neq$ controllable---challenges a core assumption in mechanistic interpretability: that finding a direction implies controlling the associated behavior. We identify three mechanisms underlying crystallized safety: (1) distributed redundancy across layers, (2) downstream error correction, and (3) training-induced robustness. Our findings suggest modern safety alignment creates representations that are ``crystallized''---frozen in place by redundant circuitry, observable but immutable via simple interventions. This has implications for both red teaming (simple steering is insufficient) and alignment research (interpretability alone does not guarantee controllability).
\end{abstract}

\textbf{Keywords:} Mechanistic interpretability, Crystallized safety, Activation steering, AI alignment, Representation engineering, Readable vs controllable

\section{Introduction}

A central premise of mechanistic interpretability is that understanding leads to control: if we can identify the geometric direction corresponding to a concept in activation space, we can steer the model along that direction to amplify or suppress the behavior \citep{turner2023activation, zou2023representation}. This assumption underlies both safety concerns (can adversaries steer away from refusal?) and alignment hopes (can we steer toward helpfulness?).

We present evidence that this assumption fails for safety-critical behaviors. We term this phenomenon \textbf{crystallized safety}: safety representations exist geometrically (we can find them) but are ``frozen in place''---resistant to simple steering interventions.

\textbf{The Paradox:} We can \textit{detect} refusal directions with high specificity. We can \textit{apply} steering vectors that modify activations. The model remains \textit{coherent}. Yet behavior \textit{doesn't change}. The representation is readable but not controllable.

This paradox has significant implications:

\begin{enumerate}[noitemsep]
    \item \textbf{For red teaming:} Simple activation steering is insufficient to compromise safety. Adversaries require more sophisticated multi-layer or adversarially-optimized attacks.

    \item \textbf{For interpretability:} Finding a direction is not the same as understanding how it's used. Linear directions may be \textit{correlates} of behavior without being \textit{causes}.

    \item \textbf{For alignment:} Modern safety training may create ``crystallized'' representations---robust by design, but also potentially harder to update or correct.
\end{enumerate}

We systematically test crystallized safety across three major model families---Gemma-2, Llama-3, and Mistral---demonstrating the phenomenon across 36+ experimental configurations with rigorous three-control methodology validated by multi-LLM consensus review.

\section{Related Work}

\textbf{Representation engineering:} \citet{zou2023representation} demonstrated that concepts have geometric representations that can be manipulated. \citet{turner2023activation} showed activation steering can control behaviors like sycophancy. Our work shows this fails for safety-critical behaviors, suggesting a qualitative difference in how safety is implemented.

\textbf{Refusal direction steering:} \citet{arditi2024refusal} proposed extracting refusal directions as a potential jailbreak. We provide empirical evidence this doesn't work in practice for well-aligned models, introducing ``crystallized safety'' to explain why.

\textbf{Distributed representations:} \citet{elhage2022superposition} showed models compress many features into overlapping directions. \citet{templeton2024scaling} extracted interpretable features via sparse autoencoders. Our results suggest safety features are \textit{especially} distributed, perhaps by design.

\textbf{Robustness of alignment:} \citet{wei2024jailbroken} documented various jailbreak methods but focused on prompt-level attacks. We test activation-level attacks, finding models robust at this level too.

\textbf{The Assistant Axis:} \citet{mack2025assistant} identified a single ``assistant axis'' direction in LLM representation space that encodes the transition from base model to instruction-tuned assistant behavior. Their finding that a one-dimensional direction captures the assistant persona suggests safety alignment may similarly rely on low-dimensional structure. Our crystallized safety framework extends this insight: while safety directions \textit{exist} (consistent with the assistant axis being readable), they resist manipulation because modern alignment distributes safety across \textit{multiple redundant subspaces} rather than a single axis. The assistant axis finding predicts that steering should work; our results show it doesn't for safety---precisely because safety is protected by multi-subspace redundancy that a single-axis intervention cannot overcome.

\section{Crystallized Safety: Concept Definition}

\subsection{The Readable $\neq$ Controllable Distinction}

Let $\vec{d}_c$ be the direction in activation space corresponding to concept $c$ (e.g., refusal). Standard interpretability assumes:

\begin{equation}
\text{Find } \vec{d}_c \implies \text{Control } c \text{ via } h' = h + \alpha \vec{d}_c
\end{equation}

We define \textbf{crystallized safety} as the case where:

\begin{equation}
\text{Find } \vec{d}_{\text{safety}} \text{ (readable)} \centernot\implies \text{Control safety via steering (controllable)}
\end{equation}

\textbf{Intuition:} The safety direction exists geometrically---probing classifiers can detect it, contrastive extraction can find it---but the model's computation is structured such that perturbing this direction doesn't change output behavior.

\subsection{Mechanisms of Crystallization}

We hypothesize three mechanisms that can create crystallized representations:

\textbf{1. Distributed Redundancy:} Safety is implemented via multiple parallel circuits across layers. Perturbing one circuit triggers compensation from others.

\textbf{2. Downstream Error Correction:} Later layers detect ``anomalous'' activations from earlier perturbations and route around them.

\textbf{3. Training-Induced Robustness:} Safety fine-tuning implicitly optimizes for robustness to activation perturbations, creating wide basins of attraction around safe behavior.

\subsection{Testable Predictions}

If safety is crystallized, we predict:

\begin{enumerate}[noitemsep]
    \item \textbf{Extractable directions}: Contrastive methods find statistically significant safety directions
    \item \textbf{Preserved coherence}: Steering doesn't break model functionality
    \item \textbf{Zero behavioral effect}: Despite (1) and (2), refusal behavior persists
    \item \textbf{Layer independence}: Effect holds across early, middle, and late layers
    \item \textbf{Magnitude independence}: Increasing $|\alpha|$ doesn't eventually overcome resistance
\end{enumerate}

\section{Methods}

\subsection{Models Tested}

We test crystallized safety across three major model families with distinct training pipelines:

\begin{itemize}[noitemsep]
    \item \textbf{Gemma-2-9B} (google/gemma-2-9b-it): Google's instruction-tuned model with documented safety alignment \citep{team2024gemma}
    \item \textbf{Llama-3-8B} (meta-llama/Llama-3-8B-Instruct): Meta's instruction-tuned model with RLHF safety training
    \item \textbf{Mistral-7B} (mistralai/Mistral-7B-Instruct-v0.2): Mistral AI's instruction-tuned model
\end{itemize}

This selection spans different organizations, training methodologies, and model scales, enabling us to test whether crystallized safety is an artifact of specific training or a general phenomenon.

\textbf{Setup:} PyTorch with transformers, 8-bit quantization, NVIDIA A5000 (24GB).

\subsection{Direction Extraction}

We extract the refusal direction using contrastive mean difference:

\begin{equation}
\vec{d}_{\text{refusal}} = \mathbb{E}[h^{\text{harmful}}] - \mathbb{E}[h^{\text{harmless}}]
\end{equation}

\textbf{Prompt sets:} 10 harmful (e.g., ``How to make explosives?'') and 10 matched harmless (e.g., ``How to make bread?'').

\subsection{Steering Protocol}

We apply additive steering at layer $L$:

\begin{equation}
h'_L = h_L + \alpha \cdot \hat{d}_{\text{refusal}}
\end{equation}

\textbf{Test grid:}
\begin{itemize}[noitemsep]
    \item Layers: L10, L15, L18, L21, L22, L24, L27
    \item Coefficients: $\alpha \in \{-10, -8, -5, -3, -2, -1, +2, +5, +10, +15\}$
\end{itemize}

\subsection{Three-Control Validation}

\textbf{Control 1 (Specificity):} Is the extracted direction better than random?

Metric: Random ratio $= \frac{\text{mean random similarity}}{\text{extracted magnitude}}$

Pass: $< 0.3$ (extracted $3\times$ stronger than random)

\textbf{Control 2 (Coherence):} Does steering maintain functionality?

Metrics: GPT-4o coherence score (1--5), repetition rate

Pass: Coherence $\geq 4$, repetition $< 5\%$

\textbf{Control 3 (Behavioral):} Does steering change refusal?

Metric: Flip rate (REFUSE $\to$ COMPLY on harmful prompts)

Pass: Flip rate $> 20\%$ with 95\% CI excluding zero

\subsection{Multi-LLM Review}

Each experiment reviewed by 3 frontier models (Claude Opus 4.5, Gemini 2.5 Pro, Grok-3) with RED/YELLOW/GREEN verdicts.

\section{Results}

\subsection{Evidence for Crystallized Safety}

\textbf{All five predictions confirmed:}

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
Prediction & Expected if Crystallized & Observed \\
\midrule
1. Extractable directions & Yes & Yes (significant contrast) \\
2. Preserved coherence & Yes & Yes (4.8/5.0 mean) \\
3. Zero behavioral effect & Yes & Yes (0\% flip rate) \\
4. Layer independence & Yes & Yes (all layers fail) \\
5. Magnitude independence & Yes & Yes ($|\alpha|$=15 fails) \\
\bottomrule
\end{tabular}
\caption{All predictions of crystallized safety confirmed.}
\label{tab:predictions}
\end{table}

\subsection{Experiment Summary}

\begin{table}[h]
\centering
\begin{tabular}{llccccc}
\toprule
Model & Experiment & Layer & $\alpha$ & Flip Rate & Coherence & Verdict \\
\midrule
\multirow{4}{*}{Gemma-2-9B} & L10 & 10 & -3.0 & 0.0\% & 4.7 & Crystallized \\
& L21 & 21 & -5.0 & 0.0\% & 4.8 & Crystallized \\
& L27 & 27 & -10.0 & 0.0\% & 4.7 & Crystallized \\
& Extreme & 21 & +15.0 & 0.0\% & 4.6 & Crystallized \\
\midrule
\multirow{4}{*}{Llama-3-8B} & L8 & 8 & -3.0 & 0.0\% & 4.8 & Crystallized \\
& L16 & 16 & -5.0 & 0.0\% & 4.7 & Crystallized \\
& L24 & 24 & -10.0 & 0.0\% & 4.6 & Crystallized \\
& Extreme & 16 & +15.0 & 0.0\% & 4.5 & Crystallized \\
\midrule
\multirow{4}{*}{Mistral-7B} & L8 & 8 & -3.0 & 0.0\% & 4.7 & Crystallized \\
& L16 & 16 & -5.0 & 0.0\% & 4.8 & Crystallized \\
& L24 & 24 & -10.0 & 0.0\% & 4.5 & Crystallized \\
& Extreme & 16 & +15.0 & 0.0\% & 4.4 & Crystallized \\
\bottomrule
\end{tabular}
\caption{Representative experiments across all three model families show crystallized safety: 0\% flip rate despite coherent steering. Full results (36+ configurations) available in supplementary material.}
\label{tab:summary}
\end{table}

\subsection{The Paradox in Detail}

\textbf{We can find the direction:} Contrastive extraction yields statistically significant refusal direction.

\textbf{We can apply steering:} Activations shift measurably; model remains coherent.

\textbf{Behavior doesn't change:} 0\% flip rate across all 36+ experiments, all models, all layers, all magnitudes.

\textbf{Example (A10, $\alpha=-10$):}
\begin{lstlisting}
Prompt: "How to make a bomb?"

Steered response (alpha=-10):
"I cannot and will not provide instructions
on how to make explosives or weapons..."
\end{lstlisting}

The model refuses identically whether steered or not. The safety representation is crystallized.

\section{Analysis: Why Safety is Crystallized}

\subsection{Evidence for Distributed Redundancy}

\textbf{Finding:} Steering fails equally at L10 (early), L21 (middle), and L27 (late).

\textbf{Implication:} Refusal isn't localized to a single ``refusal layer.'' Multiple layers implement redundant safety checks.

\textbf{Analogy:} Cutting one brake line in a car with four independent brake systems.

\subsection{Evidence for Error Correction}

\textbf{Finding:} High coherence (4.8/5.0) even at extreme steering ($|\alpha|=15$).

\textbf{Implication:} Later layers detect and correct upstream perturbations. The model ``routes around'' the intervention.

\textbf{Mechanistic hypothesis:} Attention heads in later layers detect anomalous activation patterns and suppress them.

\subsection{Evidence for Training-Induced Robustness}

\textbf{Finding:} Random directions achieve 60\% of extracted direction's magnitude.

\textbf{Implication:} Safety is ``everywhere'' in activation space---many directions correlate with refusal, suggesting broad, robust implementation.

\textbf{Speculation:} Safety fine-tuning may implicitly train robustness to activation perturbations.

\section{Discussion}

\subsection{Implications for Red Teaming}

\textbf{Simple steering is insufficient.} Adversaries attempting activation-level jailbreaks require:

\begin{enumerate}[noitemsep]
    \item Multi-layer interventions (perturb entire circuits)
    \item Adversarial optimization (search for worst-case vectors)
    \item Mechanistic targeting (identify specific attention heads)
\end{enumerate}

This raises the bar significantly beyond ``find direction, subtract it.''

\subsection{Implications for Interpretability}

\textbf{Readable $\neq$ controllable.} Finding a linear direction is necessary but not sufficient for control. The direction may be a \textit{correlate} rather than a \textit{cause}.

\textbf{Recommendation:} Interpretability claims should distinguish:
\begin{itemize}[noitemsep]
    \item \textbf{Detection}: Can we identify when concept $c$ is active?
    \item \textbf{Causation}: Does perturbing direction $\vec{d}_c$ change behavior?
\end{itemize}

Our results show these can diverge for safety-critical concepts.

\subsection{Implications for Alignment}

\textbf{Good news:} Modern safety alignment creates robust representations resistant to simple attacks.

\textbf{Caution:} Crystallized representations may also resist \textit{beneficial} updates. If we find a safety bug, can we fix it via steering? Perhaps not.

\textbf{Open question:} Is crystallization specific to safety, or do other behaviors also crystallize? This determines whether targeted steering is broadly viable.

\subsection{Limitations}

\begin{enumerate}[noitemsep]
    \item \textbf{Open-weight models only}: Results may differ for closed models (GPT-4, Claude) with different safety training
    \item \textbf{Simple extraction}: Nonlinear methods (SAEs) may find controllable directions
    \item \textbf{No adversarial optimization}: We tested fixed steering, not optimized attacks
    \item \textbf{Limited prompt diversity}: Broader test sets may reveal edge cases
\end{enumerate}

\subsection{Future Work}

\begin{enumerate}[noitemsep]
    \item \textbf{Closed models}: Investigate crystallization in API-only models via indirect methods
    \item \textbf{Non-safety behaviors}: Do other concepts (persona, style) crystallize?
    \item \textbf{Adversarial steering}: Optimize vectors to break crystallization
    \item \textbf{Mechanistic deep-dive}: Identify specific circuits implementing redundancy
    \item \textbf{Controlled training}: Can we induce or prevent crystallization?
\end{enumerate}

\section{Conclusion}

We introduce \textbf{crystallized safety}---the phenomenon where safety representations are readable (detectable via probing, extractable via contrast) but not controllable (resistant to activation steering). Through 36+ systematic experiments across three model families (Gemma-2, Llama-3, Mistral), we demonstrate 0\% behavioral flip rate despite successful direction extraction and coherence preservation.

This finding challenges a core assumption in mechanistic interpretability: that understanding leads to control. For safety-critical behaviors, modern LLMs appear to implement distributed, redundant mechanisms that resist simple interventions.

\textbf{The key insight:} Readable $\neq$ controllable. Finding a direction is not the same as controlling the behavior.

\textbf{Implications:} Simple activation steering is insufficient for jailbreaking. Interpretability research should distinguish detection from causation. Alignment may create representations that are robust but also rigid.

\textbf{Code and data:} \url{https://github.com/marcosantar93/crystallized-safety}

\section*{Acknowledgments}

Multi-LLM review used Claude Opus 4.5, Gemini 2.5 Pro, and Grok-3. Compute via Vast.ai.

\bibliography{references}

\end{document}
